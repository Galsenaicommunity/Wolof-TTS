{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10331df0-e89c-46be-bb2a-4bcc8330f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.29.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff45ad20-0d2a-4da9-8eb8-cc0ea3ea041a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31c4da-dbb2-4b78-962f-619c9a167d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/152334H/DL-Art-School\n",
    "%cd DL-Art-School\n",
    "!wget https://huggingface.co/Gatozu35/tortoise-tts/resolve/main/dvae.pth -O experiments/dvae.pth\n",
    "!wget https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth -O experiments/autoregressive.pth\n",
    "!pip install -r codes/requirements.laxed.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c09ecb-dbde-47c1-8f11-9faa8a4e52cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a990825371506c16bcf0e8167bf24ccf82f65bb6a1dbcbfcf058d76f9b197e35  ../DL-Art-School/experiments/dvae.pth\n"
     ]
    }
   ],
   "source": [
    "# @markdown Check the integrity of the dVAE checkpoint\n",
    "!sha256sum ../DL-Art-School/experiments/dvae.pth | grep a990825371506c16bcf0e8167bf24ccf82f65bb6a1dbcbfcf058d76f9b197e35 || echo \"SOMETHING IS WRONG WITH THE CHECKPOINT; REPORT THIS AS A GITHUB ISSUE AND DO NOT PROCEED\"\n",
    "# @markdown You should see the following message when verified:\n",
    "\n",
    "# @markdown > `a990825371506c16bcf0e8167bf24ccf82f65bb6a1dbcbfcf058d76f9b197e35  ../DL-Art-School/experiments/dvae.pth`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd71f9-4139-4c22-8444-89a380c54883",
   "metadata": {},
   "source": [
    "## Dataset loading and checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75faabea-c0c7-42a8-b218-d7b2b245ccb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.csv  train.txt  val.txt  wavs\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307c3f91-6021-487b-bfc2-3239433ca9d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from math import ceil\n",
    "\n",
    "def txt_file_lines(p: str) -> int:\n",
    "    return len(Path(p).read_text().strip().split('\\n'))\n",
    "\n",
    "def div_spillover(n: int, bs: int) -> int: # returns new batch size\n",
    "    epoch_steps,remain = divmod(n,bs)\n",
    "    if epoch_steps*2 > bs: return bs # don't bother optimising this stuff if epoch_steps are high\n",
    "    if not remain: return bs # unlikely but still\n",
    "\n",
    "    if remain*2 < bs: # \"easier\" to get rid of remainder -- should increase bs\n",
    "        target_bs = n//epoch_steps\n",
    "    else: # easier to increase epoch_steps by 1 -- decrease bs\n",
    "        target_bs = n//(epoch_steps+1)\n",
    "    assert n%target_bs < epoch_steps+2 # should be very few extra \n",
    "    return target_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a060ef72-bf94-4682-a273-5e6c9b8bb355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===CALCULATED SETTINGS===\n",
      "train_bs=64 val_bs=32\n",
      "val_freq=300 lr_decay_steps=[5600, 11200, 15680, 20160]\n",
      "print_freq=100 save_checkpoint_freq=300\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_TRAIN_BS = 64\n",
    "DEFAULT_VAL_BS   = 32\n",
    "#@markdown # Hyperparameter calculation\n",
    "#@markdown Run this cell to obtain suggested parameters for training\n",
    "Dataset_Training_Path = \"../../data/train.txt\" #@param {type:\"string\"}\n",
    "ValidationDataset_Training_Path = \"../../data/val.txt\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### **NOTE**: Dataset must be in the following format.\n",
    "\n",
    "#@markdown  `dataset/`\n",
    "#@markdown * ---├── `val.txt`\n",
    "#@markdown * ---├── `train.txt`\n",
    "#@markdown * ---├── `wavs/`\n",
    "\n",
    "#@markdown `wavs/` directory must contain `.wav` files.\n",
    "\n",
    "#@markdown  Example for `train.txt` and `val.txt`:\n",
    "\n",
    "#@markdown * `wavs/A.wav|Write the transcribed audio here.`\n",
    "\n",
    "#@markdown todo: actually check the dataset structure\n",
    "\n",
    "if Dataset_Training_Path == ValidationDataset_Training_Path:\n",
    "    print(\"WARNING: training dataset path == validation dataset path!!!\")\n",
    "    print(\"\\tThis is technically okay but will make all of the validation metrics useless. \")\n",
    "    print(\"it will also SUBSTANTIALLY slow down the rate of training, because validation datasets are supposed to be much smaller than training ones.\")\n",
    "\n",
    "\n",
    "training_samples = txt_file_lines(Dataset_Training_Path)\n",
    "val_samples      = txt_file_lines(ValidationDataset_Training_Path)\n",
    "\n",
    "if training_samples < 128: print(\"WARNING: very small dataset! the smallest dataset tested thus far had ~200 samples.\")\n",
    "if val_samples < 20: print(\"WARNING: very small validation dataset! val batch size will be scaled down to account\")\n",
    "\n",
    "\n",
    "if training_samples < DEFAULT_TRAIN_BS:\n",
    "    print(\"WARNING: dataset is smaller than a single batch. This will almost certainly perform poorly. Trying anyway\")\n",
    "    train_bs = training_samples\n",
    "else:\n",
    "    train_bs = div_spillover(training_samples, DEFAULT_TRAIN_BS)\n",
    "    \n",
    "if val_samples < DEFAULT_VAL_BS:\n",
    "    val_bs = val_samples\n",
    "else:\n",
    "    val_bs = div_spillover(val_samples, DEFAULT_VAL_BS)\n",
    "\n",
    "steps_per_epoch = training_samples//train_bs\n",
    "lr_decay_epochs = [20, 40, 56, 72]\n",
    "lr_decay_steps  = [steps_per_epoch * e for e in lr_decay_epochs]\n",
    "print_freq      = min(100, max(20, steps_per_epoch))\n",
    "val_freq        = save_checkpoint_freq = print_freq * 3\n",
    "\n",
    "print(\"===CALCULATED SETTINGS===\")\n",
    "print(f'{train_bs=} {val_bs=}')\n",
    "print(f'{val_freq=} {lr_decay_steps=}')\n",
    "print(f'{print_freq=} {save_checkpoint_freq=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7209b4a2-79e7-4f7a-b353-0e7d9e18f678",
   "metadata": {},
   "source": [
    "## Experiment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd4d71a5-dffa-430a-b1bd-fe37900879ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/caytu/Wolof-TTS/src/DL-Art-School\n"
     ]
    }
   ],
   "source": [
    "%cd  ../../src/DL-Art-School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ae14e9-8744-4491-89c5-f8fa00427770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-12 13:43:35--  https://raw.githubusercontent.com/152334H/DL-Art-School/master/experiments/EXAMPLE_gpt.yml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6489 (6.3K) [text/plain]\n",
      "Saving to: ‘experiments/EXAMPLE_gpt.yml’\n",
      "\n",
      "experiments/EXAMPLE 100%[===================>]   6.34K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-12 13:43:37 (72.1 MB/s) - ‘experiments/EXAMPLE_gpt.yml’ saved [6489/6489]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@markdown ##_Settings for normal users:_\n",
    "Experiment_Name         = \"Tortoise_wolof\" #@param {type:\"string\"}\n",
    "Dataset_Training_Name   = \"AntaTrain\" #@param {type:\"string\"}\n",
    "ValidationDataset_Name  = \"AntaValidation\" # this seems to be useless??? @param {type:\"string\"}\n",
    "SaveTrainingStates      = False # @param {type:\"boolean\"}\n",
    "Keep_Last_N_Checkpoints = 0 #@param {type:\"slider\", min:0, max:10, step:1}\n",
    "#@markdown * **NOTE**: 0 means \"keep all models saved\", which could potentially cause out-of-storage issues.\n",
    "#@markdown * Without training states, each model \"only\" takes up ~1.6GB. You should have ~50GB of free space to begin with.\n",
    "#@markdown * With training states, each model (pth+state) takes up ~4.9 GB; Colab will crash around ~10 undeleted checkpoints in this case.\n",
    "\n",
    "#@markdown ##_Other training parameters_\n",
    "Fp16    = False #@param {type:\"boolean\"}\n",
    "Use8bit = True #@param {type:\"boolean\"}\n",
    "#@markdown * **NOTE**: for some reason, fp16 does not seem to improve vram use when combined with 8bit [citation needed]. To be verified later...\n",
    "TrainingRate   = \"1e-5\" #@param {type:\"string\"}\n",
    "TortoiseCompat = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown * **NOTE**: TortoiseCompat introduces some breaking changes to the training process. **If you want to reproduce older models**, disable this checkbox.\n",
    "\n",
    "#@markdown ##_Calculated settings_ override\n",
    "#@markdown #####Blank entries rely on the calculated defaults from the cell above.\n",
    "#@markdown ######**Leave them blank unless you want to adjust them manually**\n",
    "TrainBS            = \"\" #@param {type:\"string\"}\n",
    "ValBS              = \"\" #@param {type:\"string\"}\n",
    "ValFreq            = \"\" #@param {type:\"string\"}\n",
    "LRDecaySteps       = \"\" #@param {type:\"string\"}\n",
    "PrintFreq          = \"\" #@param {type:\"string\"}\n",
    "SaveCheckpointFreq = \"\" #@param {type:\"string\"}\n",
    "\n",
    "def take(orig, override):\n",
    "    if override == \"\": return orig\n",
    "    return type(orig)(override)\n",
    "\n",
    "train_bs             = take(train_bs, TrainBS)\n",
    "val_bs               = take(val_bs, ValBS)\n",
    "val_freq             = take(val_freq, ValFreq)\n",
    "lr_decay_steps       = eval(LRDecaySteps) if LRDecaySteps else lr_decay_steps\n",
    "print_freq           = take(print_freq, PrintFreq)\n",
    "save_checkpoint_freq = take(save_checkpoint_freq, SaveCheckpointFreq)\n",
    "assert len(lr_decay_steps) == 4 \n",
    "gen_lr_steps = ', '.join(str(v) for v in lr_decay_steps)\n",
    "\n",
    "#@markdown #Run this cell after you finish editing the settings.\n",
    "\n",
    "!wget https://raw.githubusercontent.com/152334H/DL-Art-School/master/experiments/EXAMPLE_gpt.yml -O experiments/EXAMPLE_gpt.yml\n",
    "\n",
    "#@markdown This will apply the settings defined above to a fresh yml config file.\n",
    "import os\n",
    "!sed -i 's/batch_size: 128/batch_size: '\"$train_bs\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
    "!sed -i 's/batch_size: 64/batch_size: '\"$val_bs\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
    "!sed -i 's/val_freq: 500/val_freq: '\"$val_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
    "!sed -i 's/500, 1000, 1400, 1800/'\"$gen_lr_steps\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
    "!sed -i 's/print_freq: 100/print_freq: '\"$print_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
    "!sed -i 's/save_checkpoint_freq: 500/save_checkpoint_freq: '\"$save_checkpoint_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
    "\n",
    "!sed -i 's+CHANGEME_validation_dataset_name+'\"$ValidationDataset_Name\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
    "!sed -i 's+CHANGEME_path_to_validation_dataset+'\"$ValidationDataset_Training_Path\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
    "\n",
    "if(Fp16==True):\n",
    "    os.system(\"sed -i 's+fp16: false+fp16: true+g' ./experiments/EXAMPLE_gpt.yml\")\n",
    "!sed -i 's/use_8bit: true/use_8bit: '\"$Use8bit\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
    "\n",
    "!sed -i 's/disable_state_saving: true/disable_state_saving: '\"$SaveTrainingStates\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
    "!sed -i 's/tortoise_compat: True/tortoise_compat: '\"$TortoiseCompat\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
    "!sed -i 's/number_of_checkpoints_to_save: 0/number_of_checkpoints_to_save: '\"$Keep_Last_N_Checkpoints\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
    "\n",
    "\n",
    "!sed -i 's/CHANGEME_training_dataset_name/'\"$Dataset_Training_Name\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
    "!sed -i 's/CHANGEME_your_experiment_name/'\"$Experiment_Name\"'/g' ./experiments/EXAMPLE_gpt.yml\n",
    "!sed -i 's+CHANGEME_path_to_training_dataset+'\"$Dataset_Training_Path\"'+g' ./experiments/EXAMPLE_gpt.yml\n",
    "\n",
    "\n",
    "if (not TrainingRate==\"1e-5\"):\n",
    "    os.system(\"sed -i 's+!!float 1e-5 # CHANGEME:+!!float '\" + TrainingRate + \"' #+g' ./experiments/EXAMPLE_gpt.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e96252-02a2-4994-9da3-7f40e727a68e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba218bd7-ec4e-46a4-8332-98e31bb2e12b",
   "metadata": {},
   "source": [
    "Press the stop button for this cell when you are satisfied with the results, and have seen:\n",
    "\n",
    "`INFO:base:Saving models and training states.`\n",
    "\n",
    "If your training run saves many models, you might exceed the storage limits on the colab runtime. To prevent this, try to delete old checkpoints in `DL-Art-School/experiments/$Experiment_Name/(models|training_state)/` via the file explorer panel as the training runs. **Resuming training after a crash requires config editing,** so try to not let that happen.\n",
    "\n",
    "TODO: implement code to automatically prune useless checkpoints later && restore training states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0a82982-435b-4581-bcdd-090fdd941613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/caytu/Wolof-TTS/src/DL-Art-School/codes\n"
     ]
    }
   ],
   "source": [
    "%cd codes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcfde84b-52bb-4b85-b20f-677635e6cbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/caytu/Wolof-TTS/src/DL-Art-School/codes/trainer/base_model.py:5: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
      "Disabled distributed training.\n",
      "Path already exists. Rename it to [/home/caytu/Wolof-TTS/src/DL-Art-School/experiments/Tortoise_wolof_archived_241029-125031]\n",
      "24-10-29 12:50:31.573 - INFO:   name: Tortoise_wolof\n",
      "  model: extensibletrainer\n",
      "  scale: 1\n",
      "  gpu_ids: [0]\n",
      "  start_step: 0\n",
      "  checkpointing_enabled: True\n",
      "  fp16: False\n",
      "  use_8bit: True\n",
      "  wandb: False\n",
      "  use_tb_logger: True\n",
      "  datasets:[\n",
      "    train:[\n",
      "      name: AntaTrain\n",
      "      n_workers: 8\n",
      "      batch_size: 32\n",
      "      mode: paired_voice_audio\n",
      "      path: ../../../data/train.txt\n",
      "      fetcher_mode: ['lj']\n",
      "      phase: train\n",
      "      max_wav_length: 255995\n",
      "      max_text_length: 200\n",
      "      sample_rate: 22050\n",
      "      load_conditioning: True\n",
      "      num_conditioning_candidates: 2\n",
      "      conditioning_length: 44000\n",
      "      use_bpe_tokenizer: True\n",
      "      load_aligned_codes: False\n",
      "      data_type: img\n",
      "    ]\n",
      "    val:[\n",
      "      name: AntaValidation\n",
      "      n_workers: 1\n",
      "      batch_size: 32\n",
      "      mode: paired_voice_audio\n",
      "      path: ../../../data/val.txt\n",
      "      fetcher_mode: ['lj']\n",
      "      phase: val\n",
      "      max_wav_length: 255995\n",
      "      max_text_length: 200\n",
      "      sample_rate: 22050\n",
      "      load_conditioning: True\n",
      "      num_conditioning_candidates: 2\n",
      "      conditioning_length: 44000\n",
      "      use_bpe_tokenizer: True\n",
      "      load_aligned_codes: False\n",
      "      data_type: img\n",
      "    ]\n",
      "  ]\n",
      "  steps:[\n",
      "    gpt_train:[\n",
      "      training: gpt\n",
      "      loss_log_buffer: 500\n",
      "      optimizer: adamw\n",
      "      optimizer_params:[\n",
      "        lr: 1e-05\n",
      "        triton: False\n",
      "        weight_decay: 0.01\n",
      "        beta1: 0.9\n",
      "        beta2: 0.96\n",
      "      ]\n",
      "      clip_grad_eps: 4\n",
      "      injectors:[\n",
      "        paired_to_mel:[\n",
      "          type: torch_mel_spectrogram\n",
      "          mel_norm_file: ../experiments/clips_mel_norms.pth\n",
      "          in: wav\n",
      "          out: paired_mel\n",
      "        ]\n",
      "        paired_cond_to_mel:[\n",
      "          type: for_each\n",
      "          subtype: torch_mel_spectrogram\n",
      "          mel_norm_file: ../experiments/clips_mel_norms.pth\n",
      "          in: conditioning\n",
      "          out: paired_conditioning_mel\n",
      "        ]\n",
      "        to_codes:[\n",
      "          type: discrete_token\n",
      "          in: paired_mel\n",
      "          out: paired_mel_codes\n",
      "          dvae_config: ../experiments/train_diffusion_vocoder_22k_level.yml\n",
      "        ]\n",
      "        paired_fwd_text:[\n",
      "          type: generator\n",
      "          generator: gpt\n",
      "          in: ['paired_conditioning_mel', 'padded_text', 'text_lengths', 'paired_mel_codes', 'wav_lengths']\n",
      "          out: ['loss_text_ce', 'loss_mel_ce', 'logits']\n",
      "        ]\n",
      "      ]\n",
      "      losses:[\n",
      "        text_ce:[\n",
      "          type: direct\n",
      "          weight: 0.01\n",
      "          key: loss_text_ce\n",
      "        ]\n",
      "        mel_ce:[\n",
      "          type: direct\n",
      "          weight: 1\n",
      "          key: loss_mel_ce\n",
      "        ]\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "  networks:[\n",
      "    gpt:[\n",
      "      type: generator\n",
      "      which_model_G: unified_voice2\n",
      "      kwargs:[\n",
      "        layers: 30\n",
      "        model_dim: 1024\n",
      "        heads: 16\n",
      "        max_text_tokens: 402\n",
      "        max_mel_tokens: 604\n",
      "        max_conditioning_inputs: 2\n",
      "        mel_length_compression: 1024\n",
      "        number_text_tokens: 256\n",
      "        number_mel_codes: 8194\n",
      "        start_mel_token: 8192\n",
      "        stop_mel_token: 8193\n",
      "        start_text_token: 255\n",
      "        train_solo_embeddings: False\n",
      "        use_mel_codes_as_input: True\n",
      "        checkpointing: True\n",
      "        tortoise_compat: True\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "  path:[\n",
      "    pretrain_model_gpt: ../experiments/autoregressive.pth\n",
      "    strict_load: True\n",
      "    root: /home/caytu/Wolof-TTS/src/DL-Art-School\n",
      "    experiments_root: /home/caytu/Wolof-TTS/src/DL-Art-School/experiments/Tortoise_wolof\n",
      "    models: /home/caytu/Wolof-TTS/src/DL-Art-School/experiments/Tortoise_wolof/models\n",
      "    training_state: /home/caytu/Wolof-TTS/src/DL-Art-School/experiments/Tortoise_wolof/training_state\n",
      "    log: /home/caytu/Wolof-TTS/src/DL-Art-School/experiments/Tortoise_wolof\n",
      "    val_images: /home/caytu/Wolof-TTS/src/DL-Art-School/experiments/Tortoise_wolof/val_images\n",
      "  ]\n",
      "  train:[\n",
      "    niter: 50000\n",
      "    warmup_iter: -1\n",
      "    mega_batch_factor: 4\n",
      "    val_freq: 300\n",
      "    default_lr_scheme: MultiStepLR\n",
      "    gen_lr_steps: [5620, 11240, 15736, 20232]\n",
      "    lr_gamma: 0.5\n",
      "    ema_enabled: False\n",
      "  ]\n",
      "  eval:[\n",
      "    pure: True\n",
      "  ]\n",
      "  logger:[\n",
      "    print_freq: 100\n",
      "    save_checkpoint_freq: 300\n",
      "    visuals: ['gen', 'mel']\n",
      "    visual_debug_rate: 500\n",
      "    is_mel_spectrogram: True\n",
      "    disable_state_saving: False\n",
      "  ]\n",
      "  upgrades:[\n",
      "    number_of_checkpoints_to_save: 0\n",
      "    number_of_states_to_save: 0\n",
      "  ]\n",
      "  is_train: True\n",
      "  dist: False\n",
      "\n",
      "24-10-29 12:50:31.653 - INFO: Random seed: 2250\n",
      "24-10-29 12:50:35.129 - INFO: Number of training data elements: 17,997, iters: 563\n",
      "24-10-29 12:50:35.129 - INFO: Total epochs needed: 89 for iters 50,000\n",
      "24-10-29 12:50:35.136 - INFO: Number of val images in [AntaValidation]: 2001\n",
      "/home/caytu/Wolof-TTS/src/DL-Art-School/codes/models/lucidrains/vq.py:172: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/caytu/Wolof-TTS/src/DL-Art-School/codes/models/lucidrains/vq.py:285: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/opt/conda/lib/python3.10/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "/home/caytu/Wolof-TTS/src/DL-Art-School/codes/trainer/steps.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler(enabled=self.opt['fp16'] or opt_get(self.opt, ['grad_scaler_enabled'], False))\n",
      "/home/caytu/Wolof-TTS/src/DL-Art-School/codes/trainer/injectors/audio_injectors.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.mel_norms = torch.load(self.mel_norm_file)\n",
      "Loading from ../experiments/dvae.pth\n",
      "/home/caytu/Wolof-TTS/src/DL-Art-School/codes/utils/util.py:497: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd = torch.load(load_path, map_location=device)\n",
      "24-10-29 12:50:44.621 - INFO: Network gpt structure: DataParallel, with parameters: 421,526,786\n",
      "24-10-29 12:50:44.621 - INFO: UnifiedVoice(\n",
      "  (conditioning_encoder): ConditioningEncoder(\n",
      "    (init): Conv1d(80, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (attn): Sequential(\n",
      "      (0): AttentionBlock(\n",
      "        (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
      "        (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
      "        (attention): QKVAttentionLegacy()\n",
      "        (x_proj): Identity()\n",
      "        (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (1): AttentionBlock(\n",
      "        (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
      "        (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
      "        (attention): QKVAttentionLegacy()\n",
      "        (x_proj): Identity()\n",
      "        (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (2): AttentionBlock(\n",
      "        (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
      "        (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
      "        (attention): QKVAttentionLegacy()\n",
      "        (x_proj): Identity()\n",
      "        (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (3): AttentionBlock(\n",
      "        (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
      "        (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
      "        (attention): QKVAttentionLegacy()\n",
      "        (x_proj): Identity()\n",
      "        (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (4): AttentionBlock(\n",
      "        (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
      "        (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
      "        (attention): QKVAttentionLegacy()\n",
      "        (x_proj): Identity()\n",
      "        (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (5): AttentionBlock(\n",
      "        (norm): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
      "        (qkv): Conv1d(1024, 3072, kernel_size=(1,), stride=(1,))\n",
      "        (attention): QKVAttentionLegacy()\n",
      "        (x_proj): Identity()\n",
      "        (proj_out): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (text_embedding): Embedding(256, 1024)\n",
      "  (mel_embedding): Embedding(8194, 1024)\n",
      "  (gpt): GPT2Model(\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-29): 30 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (mel_pos_embedding): LearnedPositionEmbeddings(\n",
      "    (emb): Embedding(608, 1024)\n",
      "  )\n",
      "  (text_pos_embedding): LearnedPositionEmbeddings(\n",
      "    (emb): Embedding(404, 1024)\n",
      "  )\n",
      "  (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (text_head): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (mel_head): Linear(in_features=1024, out_features=8194, bias=True)\n",
      ")\n",
      "24-10-29 12:50:44.621 - INFO: Loading model for [../experiments/autoregressive.pth]\n",
      "/home/caytu/Wolof-TTS/src/DL-Art-School/codes/trainer/base_model.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_net = torch.load(load_path, map_location=utils.util.map_cuda_to_correct_device)\n",
      "24-10-29 12:50:45.699 - INFO: Start training from epoch: 0, iter: 0\n",
      "  0%|                                                   | 0/562 [00:00<?, ?it/s]/home/caytu/Wolof-TTS/src/DL-Art-School/codes/trainer/injectors/base_injectors.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.env['opt']['fp16'] and self.fp16_override):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "  3%|█▎                                        | 17/562 [00:21<11:44,  1.29s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/caytu/Wolof-TTS/src/DL-Art-School/codes/train.py\", line 399, in <module>\n",
      "    trainer.do_training()\n",
      "  File \"/home/caytu/Wolof-TTS/src/DL-Art-School/codes/train.py\", line 353, in do_training\n",
      "    for train_data in tq_ldr:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1465, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1491, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/_utils.py\", line 715, in reraise\n",
      "    raise exception\n",
      "TypeError: Caught TypeError in DataLoader worker process 1.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 171, in collate\n",
      "    {\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 172, in <dictcomp>\n",
      "    key: collate(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 309, in collate_int_fn\n",
      "    return torch.tensor(batch)\n",
      "TypeError: new(): invalid data type 'str'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n",
      "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n",
      "    return {\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 192, in <dictcomp>\n",
      "    key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 309, in collate_int_fn\n",
      "    return torch.tensor(batch)\n",
      "TypeError: new(): invalid data type 'str'\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 train.py -opt ../experiments/EXAMPLE_gpt.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6d7f3-518d-436d-9a56-d81740e79eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
